# Neural-Networks-Handwritten-Digits-Recognition-Backpropagation
In this repository, I have implemented neural networks and backpropagation for parameters learning to recognize handwritten digits.
My neural network has 3 layers- an input layer, a hidden layer and an output layer. In this case, our inputs are pixel values of digit images. Since the images are of size 20Ã—20 (grayscale image), this gives us 400 input layer units (excluding the extra bias unit which always outputs +1). The neural network has 25 units in the second layer (hidden layer) and 10 output units (corresponding to the 10 digit classes).
I have implemented the backpropagation algorithm for parameters learning and forward propagation to intially find the value of the hypothesis. I have implemented gradient checking to remove subtle bugs in backpropagation since it happens to be a complex algorithm; gradient checking compares a numerically computed (approximate) gradient value to the gradient value recevied from the neural network to check if they are apporximately equal. Gradient checking is computationally expensive and must be turned off after checking for once and veryfying that the algorithm is correct, otherwise our code will become very slow.
Random initialization is very important for parameters in Neural Network as setting all of the initial values of the parameter to zero or any other number leads to the problem of symmetry. For symmetry breaking, we should assign random small real numbers close to zero to thetas.
I have also used the concept of unrolling parameters to convert matrices to vectors (and vice-versa) since for computing the cost function, and for forwawrd propagation and backpropagation we need matrices (for the more effeicient vectorized methods rather than looping), but the advanced optimization algorithm requires vectors as arguments and outputs vectors as well.
